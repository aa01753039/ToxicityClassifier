# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wsfMJknTJvlfWO6AkJpf8s7y07L7KV28

# Student ID: 

**You student_id is your 7/8 digit faser number.**

This is a sample format for CE807-24-SP: Assignment . You must follow the format.
The code will have three broad sections, and additional section, if needed,


1.   Common Codes
2.   Method/model 1 Specific Codes
3.   Method/model 2 Specific Codes
4.   Other Method/model Codes, if any

**You must have `train_gen`, `test_gen` for Generative method  and `train_dis`, `test_dis` for Discriminatuve method to perform full training and testing. This will be evaluated automatically, without this your code will fail and no marked.**

You code should be proverly indended, print as much as possible, follow standard coding (https://peps.python.org/pep-0008/) and documentaion (https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.01-Help-And-Documentation.ipynb) practices.

Before each `code block/function`, you must have a `text block` which explain what code block/function is going to do. For each function/class, you need to properly document what are it's input, functionality and output.

If you are using any non-standard library, you must have command to install that, for example `pip install datasets`.

You must print `train`, `validation` and `test` performance measures.

You must also print `train` and `validation` loss in each `epoch`, wherever you are using `epoch`, say in any deep learning algorithms.

Your code must

*   To reproducibality of the results you must use a `seed`, you have to set seed in `torch`, `numpy` etc, use same seed everywhere **and your Student ID should be your seed**.
*   read dataset from './student_id/data/number/', where number is last digit of your student_id folder which will have 3 files [`train.csv`, `val.csv`, `test.csv`]
*   save model after finishing the training in './student_id/Model_Gen/' and './student_id/Model_Dis/' for Generative and Discriminative model respectively.
*   at testing time you will load models from './student_id/Model_Gen/' and './student_id/Model_Dis/'  for Generative and Discriminative model respectively. Your output file based on the test file will be named “test.csv” and you will add/modify “out_label_model_Gen” and “out_label_model_Dis” column in the existing columns from test.csv. These outputs will be generated from your trained models.
*  after testing, your output file will be named “test.csv” and you will add/modify “out_label_model_Gen” and “out_label_model_Dis” column in the existing columns from test.csv. These outputs will be generated from your trained models.




**Install and import all required libraries first before starting to code.**

Let's install all require libraries. For example, `transformers`
"""

# Install transformers library.
!pip install -q git+https://github.com/huggingface/transformers.git
# Install helper functions.
!pip install -q git+https://github.com/gmihaila/ml_things.git

!pip install gdown

"""Let's import all require libraries.
For example, `numpy`
"""

import numpy as np
import os
import pickle
import pandas as pd
import gdown
from sklearn.model_selection import train_test_split

"""**Let's put your student id as a variable, that you will use different places**"""

student_id = 2311434 # Note this is an interger and you need to input your id

"""Let's set `seed` for all libraries like `torch`, `numpy` etc as my student id"""

# set same seeds for all libraries

#numpy seed
np.random.seed(student_id)

"""# Common Codes

In this section you will write all common codes, for examples


*   Data read
*   Command Line argument reading
*   Performance Matrics
*   Print Dataset Statistics
*   Saving model and output
*   Loading Model and output
*   etc

**Let's first allow the GDrive access and set data and model paths**

For examples,

student_id = 12345670

set GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = ‘./CE807-24-SP/Lab10/’ in your GDrive

now set all global variable,


Sample output directory and file structure: https://drive.google.com/drive/folders/1okgSzgGiwPYYFp7NScEt9MNVolOlld1d?usp=share_link
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# Add your code to initialize GDrive and data and models paths

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = './CE807-24-SP/Lab10/'
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))

DATA_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'data', '4') # Make sure to replace 0 with last digit of your student Regitration number
train_file = os.path.join(DATA_PATH, 'train.csv')
print('Train file: ', train_file)

val_file = os.path.join(DATA_PATH, 'valid.csv')
print('Validation file: ', val_file)

test_file = os.path.join(DATA_PATH, 'test.csv')
print('Test file: ', test_file)

train2_file = os.path.join(DATA_PATH, 'train_more.csv')
print('Train deep file: ', train2_file)

train_wiki_file = os.path.join(DATA_PATH, 'train_wikipedia.csv')
print('Train deep file: ', train_wiki_file)

val_wiki_file = os.path.join(DATA_PATH, 'validation_wikipedia.csv')
print('Validation deep file: ', val_wiki_file)

sota_wiki_file = os.path.join(DATA_PATH, 'sota_predictions.txt')
print('SoTA predictions on wikipedia validation file: ', sota_wiki_file)

sota_file = os.path.join(DATA_PATH, 'sota_predictions2.txt')
print('SoTA predictions on validation file: ', sota_file)

MODEL_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'model', str(student_id)) # Make sure to use your student Regitration number
MODEL_Gen_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Gen') # Model Generative directory
print('Model Generative directory: ', MODEL_Gen_DIRECTORY)

MODEL_Gen_File = MODEL_Gen_DIRECTORY + '.zip'

TOKEN_Gen_DIRECTORY = os.path.join(MODEL_PATH, 'Token_Gen') # Model Generative directory
print('Model Generative directory: ', TOKEN_Gen_DIRECTORY)

TOKEN_Gen_File = TOKEN_Gen_DIRECTORY + '.zip'


MODEL_Dis_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Dis') # Model Discriminative directory
print('Model Discriminative directory: ', MODEL_Dis_DIRECTORY)

MODEL_Dis_File = MODEL_Dis_DIRECTORY + '.zip'

TOKEN_Dis_DIRECTORY = os.path.join(MODEL_PATH, 'Token_Dis') # Model Generative directory
print('Model Discriminative directory: ', TOKEN_Dis_DIRECTORY)

TOKEN_Dis_File = TOKEN_Dis_DIRECTORY + '.zip'

MODEL_Combine_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Combine') # Model Generative directory
print('Model Combination directory: ', MODEL_Combine_DIRECTORY)

MODEL_Combine_File = MODEL_Combine_DIRECTORY + '.zip'

# more data obtained from https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/discussion

"""Let's see train file"""

train_df = pd.read_csv(train_file)
train_df.head()

"""Let's show you a sample output file. Notice all fields, `out_label` is your model's output for that `tweet` and `id`

We are going to use different performance matrics like Accuracy, Recall (macro), Precision (macro), F1 (macro) and Confusion Matrix for the performance evaluation. We will print all the matrics and display Confusion Matrix with proper X & Y axis labels
"""

from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score, confusion_matrix
from joblib import dump, load

def compute_performance(y_true, y_pred):
    """
    prints different performance matrics like  Accuracy, Recall (macro), Precision (macro), and F1 (macro).
    This also display Confusion Matrix with proper X & Y axis labels.
    Also, returns F1 score

    Args:
        y_true: numpy array or list
        y_pred: numpy array or list


    Returns:
        float
    """

    ##########################################################################
    #                     TODO: Implement this function                      #
    ##########################################################################
    # Replace "pass" statement with your code
    cm = confusion_matrix(y_true,y_pred)
    accuracy = accuracy_score(y_true,y_pred)
    recall = recall_score(y_true,y_pred,average='macro')
    precision = precision_score(y_true,y_pred,average='macro')
    score = f1_score(y_true, y_pred, average='macro')

    print('Accuracy: ',round(accuracy*100,3),'%')
    print('Recall: ',round(recall*100,3),'%')
    print('Precision: ',round(precision*100,3),'%')
    print('F1-Score: ',round(score*100,3),'%')
    print(cm)
    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################
    return score

def save_model(model,model_dir:str):
  # save the model to disk
  # Check if the Model directory exists

  # Note you might have to modify this based on your requirement

  if not os.path.exists(model_dir):
      # Create the directory if it doesn't exist
      os.makedirs(model_dir)
      print(f"Directory '{model_dir}' created successfully.")
  else:
      print(f"Directory '{model_dir}' already exists.")

  model.save_pretrained(model_dir)


  print('Saved model to ', model_dir)

  return model_dir

def save_model_combine(model,model_dir:str):
  # save the model to disk
  # Check if the Model directory exists

  # Note you might have to modify this based on your requirement

    if not os.path.exists(model_dir):
        # Create the directory if it doesn't exist
        os.makedirs(model_dir)
        print(f"Directory '{model_dir}' created successfully.")
    else:
        print(f"Directory '{model_dir}' already exists.")

    dump(model, model_dir+'/model.joblib')


    print('Saved model to ', model_dir+'/model.joblib')

    return model_dir

def load_model(model_file:str,token_file:str):

    # load model from disk

    # Load the tokenizer and model from the saved directory

    tokenizer = AutoTokenizer.from_pretrained(token_file)

    model = AutoModelForSequenceClassification.from_pretrained(
        model_file)

    print('Loaded model from ', model_file)
    print('Loaded tokenizer from ', token_file)

    return model,tokenizer

def load_model_combine(model_file:str):
    # load model from joblib

    # Load the  model from the saved directory

    model_dir = model_file +'/model.joblib'
    model = clf = load(model_dir)

    print('Loaded model from ', model_dir)

    return model

"""# Let's download GDrive Link into a directory"""

import requests

def extract_file_id_from_url(url):
    # Extract the file ID from the URL
    file_id = None
    if 'drive.google.com' in url:
        file_id = url.split('/')[-2]
    elif 'https://docs.google.com' in url:
        file_id = url.split('/')[-1]

    return file_id

def download_file_from_drive(file_id, file_path):
    # Construct the download URL
    download_url = f"https://drive.google.com/uc?id={file_id}"

    # Download the file
    response = requests.get(download_url)
    if response.status_code == 200:
        with open(file_path, 'wb') as f:
            f.write(response.content)
        print("File downloaded successfully!",file_path)
    else:
        print("Failed to download the file.")

def download_zip_file_from_link(file_url,file_path):

  file_id = extract_file_id_from_url(file_url)
  if file_id:
      download_file_from_drive(file_id, file_path)
  else:
      print("Invalid Google Drive URL.")

"""# Zip and Unzip a GDrive File"""

import zipfile
import shutil
import os

# Function to zip a directory
def zip_directory(directory, zip_filename):
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(directory):
            for file in files:
                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory, '..')))
        print('Created a zip file',zip_filename)

# Function to unzip a zip file
def unzip_file(zip_filename, extract_dir):
    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)
    print('Extracted a zip file to',extract_dir)

# Example usage:
# directory_to_zip = 'path/to/your/directory'
# zip_filename = 'output_zipfile.zip'

# # Zip the directory
# zip_directory(directory_to_zip, zip_filename)

# # Unzip the zip file
# extract_dir = 'path/to/extract'
# unzip_file(zip_filename, extract_dir)

"""# Get Sharable link of your Zip file in Gdrive"""

!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials


def get_gdrive_link(file_path):
    # Authenticate and create PyDrive client
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)

    # Find the file in Google Drive
    file_name = file_path.split('/')[-1]
    file_list = drive.ListFile({'q': f"title='{file_name}'"}).GetList()

    # Get the file ID and generate the shareable link
    if file_list:
        file_id = file_list[0]['id']
        gdrive_link = f"https://drive.google.com/file/d/{file_id}/view?usp=sharing"
        return gdrive_link
    else:
        return "File not found in Google Drive"

def get_shareable_link(url):

    file_id = extract_file_id_from_url(url)

    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)

    try:
        file_obj = drive.CreateFile({'id': file_id})
        file_obj.FetchMetadata()
        file_obj.InsertPermission({
            'type': 'anyone',
            'value': 'anyone',
            'role': 'reader'
        })

        # Get the shareable link
        return file_obj['alternateLink']
    except Exception as e:
        print("Error:", e)
        return None

# if __name__ == "__main__":
#     # Replace 'YOUR_FILE_ID' with the ID of the file you want to share
#     file_id = 'YOUR_FILE_ID'
#     shareable_link = get_shareable_link(file_id)
#     if shareable_link:
#         print("Shareable link:", shareable_link)
#     else:
#         print("Failed to generate shareable link.")

def download_model(MODEL_PATH,model_gdrive_link,token_gdrive_link):
    """
      Downloads a model and its associated tokenizer from Google Drive and unzips them.

      Args:
      MODEL_PATH (str): The base path where model and tokenizer files will be saved.
      model_gdrive_link (str): The Google Drive link to the model file.
      token_gdrive_link (str): The Google Drive link to the tokenizer file.

      Returns:
      tuple: A tuple containing two strings:
      - The full path to the downloaded model file.
      - The full path to the downloaded tokenizer file.

      Raises:
      ImportError: If the gdown library is not installed.

      Additional Information:
      - The function assumes that both model and tokenizer files are in a zip format.
      - It uses the external library gdown to download files from Google Drive.
      - It prints progress messages to the console during the download and unzipping process.
      - It includes a 5-second delay after starting each download to potentially simulate a more realistic download time.
    """


    print('\n Start by downloading model')

    test_model_path = MODEL_PATH+'/test/'
    test_model_file = gdown.download(model_gdrive_link, test_model_path, quiet=False,fuzzy=True)
    print('Unzipping...')
    time.sleep(5)

    unzip_file(test_model_file, test_model_path)
    print('\n Model is downloaded to ',test_model_path)
    model_file = test_model_path+test_model_file.split('.')[-2].split('/')[-1]
    print('\n Model file: ',model_file)

    print('\n Start by downloading tokenizer')

    test_token_path = MODEL_PATH+'/test/'
    test_token_file = gdown.download(token_gdrive_link, test_token_path, quiet=False,fuzzy=True)
    print('Unzipping...')
    time.sleep(5)

    unzip_file(test_token_file, test_token_path)
    print('\n Tokenizer is downloaded to ',test_token_path)
    token_file = test_token_path+test_token_file.split('.')[-2].split('/')[-1]
    print('\n Tokenizer file: ',token_file)

    return model_file,token_file

def download_combined_model(MODEL_PATH,model_gdrive_link):
    """
    Downloads a model from Google Drive and unzips it.

    Args:
    MODEL_PATH (str): The base path where the model file will be saved.
    model_gdrive_link (str): The Google Drive link to the combined model file.

    Returns:
    str: The full path to the downloaded model file.

    Raises:
    ImportError: If the gdown library is not installed.

    Additional Information:
    - The function assumes that the combined model file is in a zip format.
    - It uses the external library gdown to download files from Google Drive.
    - It prints progress messages to the console during the download and unzipping process.
    - It includes a 5-second delay after starting the download to potentially simulate a more realistic download time.
    """
    print('\n Start by downloading model')

    test_model_path = MODEL_PATH+'/test/'
    test_model_file = gdown.download(model_gdrive_link, test_model_path, quiet=False,fuzzy=True)
    print('Unzipping...')
    time.sleep(5)

    unzip_file(test_model_file, test_model_path)
    print('\n Model is downloaded to ',test_model_path)
    model_file = test_model_path+test_model_file.split('.')[-2].split('/')[-1]
    print('\n Model file: ',model_file)

    return model_file

"""# Data processing functions"""

def clean_full_text(text:str):
    """
    Cleans the input text by performing several regex substitutions:

    - Removes 'SDATA_4 :' and ' : EDATA_4' from the text.
    - Replaces 'NEWLINE_TOKEN' with a single space.
    - Removes all extra whitespace characters including new lines and tabs.
    - Removes all non-alphabetical characters.
    - Converts the text to lowercase.

    Parameters:
    text (str): The text to be cleaned.

    Returns:
    str: The cleaned text.
    """
    # Remove 'SDATA_4 :' and ' : EDATA_4'
    text = re.sub(r'SDATA_4\s*:\s*|\s*:\s*EDATA_4', '', text)
    # Replace 'NEWLINE_TOKEN' with a space
    text = re.sub(r'NEWLINE_TOKEN', ' ', text)
    # Remove extra whitespace characters
    text = re.sub(r'\s+', ' ', text)
    # Convert newline characters to spaces
    text = re.sub(r'\n', ' ', text)
    # Remove all non-alphabetical characters
    text = re.sub(r'[^a-zA-Z]', '', text)
    # Convert text to lowercase
    text = text.lower()

    return text

def clean_text(text):
    """
    Cleans the input text by applying a series of regular expression substitutions to:

    - Remove specific markers ('SDATA_4 :', ' : EDATA_4').
    - Replace 'NEWLINE_TOKEN' and 'User:' prefixes with a space.
    - Remove 're:' prefixes.
    - Strip URLs.
    - Replace multiple spaces and newline characters with a single space.
    - Remove certain punctuation marks like '==' and '`'.
    - Convert the text to lowercase.

    This function is useful for preprocessing text for natural language processing tasks.

    Parameters:
    text (str): The string of text to be cleaned.

    Returns:
    str: The cleaned and processed text.
    """
    # Remove specific markers
    text = re.sub(r'SDATA_4\s*:\s*|\s*:\s*EDATA_4', '', text)
    # Replace 'NEWLINE_TOKEN' and user mentions with a space
    text = re.sub(r'NEWLINE_TOKEN', ' ', text)
    text = re.sub(r'User:', ' ', text)
    # Remove 're:' prefix
    text = re.sub(r're:', ' ', text)
    # Strip URLs
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    # Replace multiple spaces and newlines with a single space
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n', ' ', text)
    # Remove '==' and '`'
    text = re.sub(r'==', '', text)
    text = re.sub(r'`', '', text)
    # Convert text to lowercase
    text = text.lower()

    return text

def load_data(path):
    """
    Loads and preprocesses data from a CSV file at the specified path. The function:

    - Reads the data from the CSV file.
    - Renames columns to standardize them ('id' to 'comment_id', 'comment_text' to 'comment',
      and 'toxic' to 'toxicity').
    - Drops unnecessary columns like 'severe_toxic', 'obscene', 'threat', 'insult',
      and 'identity_hate'.
    - Cleans the 'comment' column text using the 'clean_text' function.
    - Maps the 'toxicity' column values from numerical to categorical ('no toxic' and 'toxic').

    Parameters:
    path (str): The file path of the CSV data to be loaded.

    Returns:
    pandas.DataFrame: A DataFrame containing the cleaned and processed data.
    """
    # Load extra data
    data = pd.read_csv(path).rename({'id':'comment_id', 'comment_text':'comment', 'toxic':'toxicity'}, axis=1).drop(columns=['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])

    # Clean the comment text in the datasets
    data['comment'] = data['comment'].apply(clean_text)

    # Map to 'toxic' and 'no toxic'
    data['toxicity'] = data['toxicity'].map({0:'no toxic', 1:'toxic'})

    return data

def remove_rows(df, df2):
    """
    Removes rows from the first dataframe (df) that have similar text content to the second dataframe (df2)
    based on the first 15 characters of cleaned text. This function is used to ensure that the new data
    (df) does not contain any data already present in the train, test, or validation datasets (df2).

    Steps:
    - Cleans the text in both dataframes using `clean_full_text`.
    - Creates a 'short_text' column in both dataframes with the first 15 characters of the cleaned text.
    - Identifies and removes rows in df that have 'short_text' values matching any in df2.
    - Drops the 'short_text' and 'text' columns from the cleaned data.

    Parameters:
    df (pandas.DataFrame): The primary dataframe from which rows will be removed.
    df2 (pandas.DataFrame): The reference dataframe that provides the criteria for removal.

    Returns:
    pandas.DataFrame: A DataFrame with rows removed from df based on the similarity criteria with df2.
    """
    data = df.copy()
    data2 = df2.copy()

    data['text'] = data['comment_text'].apply(clean_full_text)
    data2['text'] = data2['comment'].apply(clean_full_text)

    # Use only the first 15 characters as id
    data["short_text"] = data["text"].apply(lambda text: text[:15])
    data2["short_text"] = data2["text"].apply(lambda text: text[:15])

    data_cleaned = data[~data['short_text'].isin(data2['short_text'])]

    data_cleaned.drop(columns=['short_text', 'text'], inplace=True)
    return data_cleaned

def join_data(train_file, val_file, test_file, more_data):
    """
    Loads data from specified CSV files and then removes any rows from the 'more_data' dataset
    that are similar to rows in the 'train', 'val', or 'test' datasets. This is done to ensure
    that the 'more_data' set is distinct and does not contain any data already present in the
    training, validation, or testing sets.

    The similarity check and removal of rows is performed by the `remove_rows` function.

    Parameters:
    train_file (str): File path to the training dataset CSV.
    val_file (str): File path to the validation dataset CSV.
    test_file (str): File path to the testing dataset CSV.
    more_data (str): File path to the additional data CSV that needs to be cleaned of any overlapping entries.

    Returns:
    pandas.DataFrame: The cleaned 'more_data' DataFrame, with rows similar to those in the
                      'train', 'val', or 'test' datasets removed.
    """
    train = pd.read_csv(train_file)
    val = pd.read_csv(val_file)
    test = pd.read_csv(test_file)
    df = pd.read_csv(more_data)

    data = remove_rows(df, train)
    data = remove_rows(data, val)
    data = remove_rows(data, test)

    return data

def split_data(train_df, more_data):
    """
    Combines and shuffles data from 'train_df' and 'more_data' DataFrames, then splits the combined
    data into training and validation sets. Column names are standardized before combining to ensure
    consistency. The split is stratified by the 'toxic' column to maintain the proportion of classes.

    Parameters:
    train_df (pandas.DataFrame): DataFrame containing the initial training data.
    more_data (pandas.DataFrame): DataFrame containing additional data to be included in the training and validation sets.

    Returns:
    tuple: A tuple containing two DataFrames, the first being the new training set and the second the validation set.
           Both sets include the 'toxic' column as the target variable.
    """
    # Copy the data to avoid modifying the original DataFrames
    data = more_data.copy()
    train = train_df.copy()

    # Standardize column names
    train.rename({'comment_id': 'id', 'comment': 'comment_text', 'toxicity': 'toxic'}, axis=1, inplace=True)

    # Combine and shuffle the datasets
    df = pd.concat([data, train])
    df = df.sample(frac=1, random_state=student_id).reset_index(drop=True)

    # Split the data into train and validation sets
    X_train, X_val, y_train, y_val = train_test_split(
        df.drop(columns=['toxic']),
        df['toxic'],
        random_state=student_id,
        test_size=0.2,
        stratify=df['toxic']
    )

    # Create DataFrames for train and validation sets
    train_df = X_train.copy()
    train_df['toxic'] = y_train

    val_df = X_val.copy()
    val_df['toxic'] = y_val

    return train_df, val_df

def process_data(train_file, val_file, test_file, train2_file):
    """
    Processes data by first joining additional training data from 'train2_file' with the existing
    training, validation, and testing datasets to ensure it does not include overlapping entries.
    It then splits the resulting combined dataset into new training and validation sets.

    Parameters:
    train_file (str): File path to the main training dataset CSV.
    val_file (str): File path to the validation dataset CSV.
    test_file (str): File path to the testing dataset CSV.
    train2_file (str): File path to the additional training data CSV.

    Returns:
    tuple: A tuple containing two pandas DataFrames, the first being the new training set and the
           second being the new validation set. The data is cleaned and split to ensure it's ready
           for model training and validation.
    """
    # Join the additional data with the existing datasets, excluding overlapping entries
    data = join_data(train_file, val_file, test_file, train2_file)

    # Load the main training dataset
    train = pd.read_csv(train_file)

    # Split the combined dataset into new training and validation sets
    train_df, val_df = split_data(train, data)

    return train_df, val_df

def upload_data(train_file, val_file, test_file, train2_file, train_2_file, val_2_file):
    """
    Processes and saves new training and validation datasets. This function takes the original
    training, validation, and testing files along with an additional training file, processes them to
    ensure they are clean and without overlapping data, and then splits the processed data into
    new training and validation sets which are saved to specified locations.

    Parameters:
    train_file (str): File path to the main training dataset CSV.
    val_file (str): File path to the validation dataset CSV.
    test_file (str): File path to the testing dataset CSV.
    train2_file (str): File path to the additional training data CSV.
    train_2_file (str): Destination file path to save the new processed training dataset.
    val_2_file (str): Destination file path to save the new processed validation dataset.

    The function prints out the progress and confirms the saving of the new datasets. No return value.
    """
    print("Cleaning and processing data...")
    print("Obtaining more data and merging dataframes for a more complete dataset...")

    # Process the data and obtain new training and validation sets
    train, valid = process_data(train_file, val_file, test_file, train2_file)

    # Save the new training set to the specified file
    train.to_csv(train_2_file, index=False)
    print("New train dataset saved to: ", train_2_file)

    # Save the new validation set to the specified file
    valid.to_csv(val_2_file, index=False)
    print("New validation dataset saved to: ", val_2_file)

"""# Method Generative Start

In this section you will write all details of your Method 1.

You will have to enter multiple `code` and `text` cell.

Your code should follow the standard ML pipeline


*   Data reading
*   Data clearning, if any
*   Convert data to vector/tokenization/vectorization
*   Model Declaration/Initialization/building
*   Training and validation of the model using training and validation dataset
*   Save the trained model
*   Load and Test the model on testing set
*   Save the output of the model


You could add any other step(s) based on your method's requirement.

After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of `function`.

"""

#code inspired in https://gmihaila.github.io/tutorial_notebooks/gpt2_finetune_classification/#complete-tutorial-on-how-to-use-gpt2-for-text-classification

import io
import time
import torch
import re

import string
from tqdm.notebook import tqdm
from torch.utils.data import Dataset, DataLoader, TensorDataset
from ml_things import plot_dict, plot_confusion_matrix, fix_text
from sklearn.metrics import classification_report, accuracy_score
from transformers import (set_seed,
                          TrainingArguments,
                          Trainer,
                          GPT2Config,
                          GPT2Tokenizer,
                          AutoTokenizer,
                          AutoConfig,
                          AdamW,
                          get_linear_schedule_with_warmup,
                          AutoModelForSequenceClassification,
                          GPT2ForSequenceClassification
                          )

class ToxicDataset(Dataset):
  """PyTorch Dataset class for loading data.

  This is where the data parsing happens.

  This class is built with reusability in mind: it can be used as is as.

  Arguments:

    path (:obj:`str`):
        Path to the data partition.

  """

  def __init__(self, path, use_tokenizer):

    data = load_data(path)
    self.texts = data['comment'].apply(fix_text).to_numpy()
    self.labels = data['toxicity'].to_numpy()

    # Number of exmaples.
    self.n_examples = len(self.labels)


    return

  def __len__(self):
    r"""When used `len` return the number of examples.

    """

    return self.n_examples

  def __getitem__(self, item):
    r"""Given an index return an example from the position.

    Arguments:

      item (:obj:`int`):
          Index position to pick an example to return.

    Returns:
      :obj:`Dict[str, str]`: Dictionary of inputs that contain text and
      asociated labels.

    """

    return {'text':self.texts[item],
            'label':self.labels[item]}



class Gpt2ClassificationCollator(object):
    r"""
    Data Collator used for GPT2 in a classificaiton task.

    It uses a given tokenizer and label encoder to convert any text and labels to numbers that
    can go straight into a GPT2 model.

    This class is built with reusability in mind: it can be used as is as long
    as the `dataloader` outputs a batch in dictionary format that can be passed
    straight into the model - `model(**batch)`.

    Arguments:

      use_tokenizer (:obj:`transformers.tokenization_?`):
          Transformer type tokenizer used to process raw text into numbers.

      labels_ids (:obj:`dict`):
          Dictionary to encode any labels names into numbers. Keys map to
          labels names and Values map to number associated to those labels.

      max_sequence_len (:obj:`int`, `optional`)
          Value to indicate the maximum desired sequence to truncate or pad text
          sequences. If no value is passed it will used maximum sequence size
          supported by the tokenizer and model.

    """

    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):

        # Tokenizer to be used inside the class.
        self.use_tokenizer = use_tokenizer
        # Check max sequence length.
        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len
        # Label encoder used inside the class.
        self.labels_encoder = labels_encoder

        return

    def __call__(self, sequences):
        r"""
        This function allowes the class objesct to be used as a function call.
        Sine the PyTorch DataLoader needs a collator function, I can use this
        class as a function.

        Arguments:

          item (:obj:`list`):
              List of texts and labels.

        Returns:
          :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.
          It holddes the statement `model(**Returned Dictionary)`.
        """

        # Get all texts from sequences list.
        texts = [sequence['text'] for sequence in sequences]
        # Get all labels from sequences list.
        labels = [sequence['label'] for sequence in sequences]
        # Encode all labels using label encoder.
        labels = [self.labels_encoder[label] for label in labels]
        # Call tokenizer on all texts to convert into tensors of numbers with
        # appropriate padding.
        inputs = self.use_tokenizer(text=texts, return_tensors="pt", padding=True, truncation=True,  max_length=self.max_sequence_len)
        # Update the inputs with the associated encoded labels as tensor.
        inputs.update({'labels':torch.tensor(labels)})

        return inputs


def train(dataloader, optimizer_, scheduler_, device_):
  r"""
  Train pytorch model on a single pass through the data loader.

  It will use the global variable `model` which is the transformer model
  loaded on `_device` that we want to train on.

  This function is built with reusability in mind: it can be used as is as long
    as the `dataloader` outputs a batch in dictionary format that can be passed
    straight into the model - `model(**batch)`.

  Arguments:

      dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):
          Parsed data into batches of tensors.

      optimizer_ (:obj:`transformers.optimization.AdamW`):
          Optimizer used for training.

      scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`):
          PyTorch scheduler.

      device_ (:obj:`torch.device`):
          Device used to load tensors before feeding to model.

  Returns:

      :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted
        Labels, Train Average Loss].
  """

  # Use global variable for model.
  global model

  # Tracking variables.
  predictions_labels = []
  true_labels = []
  # Total loss for this epoch.
  total_loss = 0

  # Put the model into training mode.
  model.train()

  # For each batch of training data...
  for batch in tqdm(dataloader, total=len(dataloader)):

    # Add original labels - use later for evaluation.
    true_labels += batch['labels'].numpy().flatten().tolist()

    # move batch to device
    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}

    # Always clear any previously calculated gradients before performing a
    # backward pass.
    model.zero_grad()

    # Perform a forward pass (evaluate the model on this training batch).
    # This will return the loss (rather than the model output) because we
    # have provided the `labels`.
    # The documentation for this a bert model function is here:
    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification
    outputs = model(**batch)

    # The call to `model` always returns a tuple, so we need to pull the
    # loss value out of the tuple along with the logits. We will use logits
    # later to calculate training accuracy.
    loss, logits = outputs[:2]

    # Accumulate the training loss over all of the batches so that we can
    # calculate the average loss at the end. `loss` is a Tensor containing a
    # single value; the `.item()` function just returns the Python value
    # from the tensor.
    total_loss += loss.item()

    # Perform a backward pass to calculate the gradients.
    loss.backward()

    # Clip the norm of the gradients to 1.0.
    # This is to help prevent the "exploding gradients" problem.
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    # Update parameters and take a step using the computed gradient.
    # The optimizer dictates the "update rule"--how the parameters are
    # modified based on their gradients, the learning rate, etc.
    optimizer_.step()

    # Update the learning rate.
    scheduler_.step()

    # Move logits and labels to CPU
    logits = logits.detach().cpu().numpy()

    # Convert these logits to list of predicted labels values.
    predictions_labels += logits.argmax(axis=-1).flatten().tolist()

  # Calculate the average loss over the training data.
  avg_epoch_loss = total_loss / len(dataloader)

  # Return all true labels and prediction for future evaluations.
  return true_labels, predictions_labels, avg_epoch_loss



def validation(dataloader, device_):
  r"""Validation function to evaluate model performance on a
  separate set of data.

  This function will return the true and predicted labels so we can use later
  to evaluate the model's performance.

  This function is built with reusability in mind: it can be used as is as long
    as the `dataloader` outputs a batch in dictionary format that can be passed
    straight into the model - `model(**batch)`.

  Arguments:

    dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):
          Parsed data into batches of tensors.

    device_ (:obj:`torch.device`):
          Device used to load tensors before feeding to model.

  Returns:

    :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted
        Labels, Train Average Loss]
  """

  # Use global variable for model.
  global model

  # Tracking variables
  predictions_labels = []
  true_labels = []
  #total loss for this epoch.
  total_loss = 0

  # Put the model in evaluation mode--the dropout layers behave differently
  # during evaluation.
  model.eval()

  # Evaluate data for one epoch
  for batch in tqdm(dataloader, total=len(dataloader)):

    # add original labels
    true_labels += batch['labels'].numpy().flatten().tolist()

    # move batch to device
    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}

    # Telling the model not to compute or store gradients, saving memory and
    # speeding up validation
    with torch.no_grad():

        # Forward pass, calculate logit predictions.
        # This will return the logits rather than the loss because we have
        # not provided labels.
        # token_type_ids is the same as the "segment ids", which
        # differentiates sentence 1 and 2 in 2-sentence tasks.
        # The documentation for this `model` function is here:
        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification
        outputs = model(**batch)

        # The call to `model` always returns a tuple, so we need to pull the
        # loss value out of the tuple along with the logits. We will use logits
        # later to to calculate training accuracy.
        loss, logits = outputs[:2]

        # Move logits and labels to CPU
        logits = logits.detach().cpu().numpy()

        # Accumulate the training loss over all of the batches so that we can
        # calculate the average loss at the end. `loss` is a Tensor containing a
        # single value; the `.item()` function just returns the Python value
        # from the tensor.
        total_loss += loss.item()

        # get predicitons to list
        predict_content = logits.argmax(axis=-1).flatten().tolist()

        # update list
        predictions_labels += predict_content

  # Calculate the average loss over the training data.
  avg_epoch_loss = total_loss / len(dataloader)

  # Return all true labels and prediciton for future evaluations.
  return true_labels, predictions_labels, avg_epoch_loss

def inference_gen(input_text, model, tokenizer, device):
    """
    Executes model inference for generating outputs based on the provided input text. This function
    cleans the text, tokenizes it, loads it into a DataLoader, and then feeds it through the model
    to obtain logits as outputs.

    Parameters:
    input_text (str): The text input for which inference is to be performed.
    model: The machine learning model to use for inference, expected to be a pre-trained model
           such as BERT or GPT-2.
    tokenizer: The tokenizer that corresponds to the model, used for converting the input text
               into a format suitable for the model.
    device: The computation device ('cuda' or 'cpu') where the model will run.

    Returns:
    numpy.ndarray: The logits from the model output, representing unnormalized predictions.

    This function processes the input through cleaning, tokenizing, and passing it to the model.
    The output logits are typically used to generate predictions or further processed to obtain
    probability distributions.
    """
    # Clean the input text
    user_input = clean_text(input_text)

    # Tokenize the cleaned text
    user_encodings = tokenizer(
        user_input, truncation=True, padding=True, return_tensors="pt")

    # Create a dataset and loader for the tokenized text
    user_dataset = TensorDataset(
        user_encodings['input_ids'], user_encodings['attention_mask'])

    user_loader = DataLoader(user_dataset, batch_size=1, shuffle=False)

    # Put the model in evaluation mode
    model.eval()
    with torch.no_grad():
        for batch in user_loader:
            # Move batch to device
            input_ids, attention_mask = [t.to(device) for t in batch]
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            # Detach logits from the graph and move to CPU
            logits = logits.detach().cpu().numpy()

    return logits

"""## Training Generative Method Code
Your test code should be a stand alone code that must take `train_file`, `val_file`,  and `model_dir` as input. You could have other things as also input, but these three are must. You would load both files, and train using the `train_file` and validating using the `val_file`. You will `print` / `display`/ `plot` all performance metrics, loss(if available) and save the output model in the `model_dir`.

Note that at the testing time, you need to use the same pre-processing and model. So, it would be good that you make those as seperate function/pipeline whichever it the best suited for your method. Don't copy-paste same code twice, make it a fucntion/class whichever is best.
"""

def train_Gen(train_file, val_file, model_dir, token_dir):
    """
    Takes train_file, val_file and model_dir as input.
    It trained on the train_file datapoints, and validate on the val_file datapoints.
    While training and validating, it print different evaluataion metrics and losses, wheverever necessary.
    After finishing the training, it saved the best model in the model_dir.

    ADD Other arguments, if needed.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory
        token_dir: Tokenizer output Directory



    """

    ##########################################################################
    #                     TODO: Implement this function                      #
    ##########################################################################
    # Replace "pass" statement with your code
    # Get model configuration.
    # Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).
    epochs = 3

    # Number of batches - depending on the max sequence length and GPU memory.
    # For 512 sequence length batch of 10 works without cuda memory issues.
    # For small sequence length can try batch of 32 or higher.
    batch_size = 16

    # Pad or truncate text sequences to a specific length
    # if `None` it will use maximum sequence of word piece tokens allowed by model.
    max_length = 100

    # Look for gpu to use. Will use `cpu` by default if no gpu found.
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Name of transformers model - will use already pretrained model.
    # Path of transformer model - will load your own model from local disk.
    model_name_or_path = 'gpt2'

    # Dictionary of labels and their id - this will be used to convert.
    # String labels to number ids.
    labels_ids = {'no toxic': 0, 'toxic': 1}

    # How many labels are we using in training.
    # This is used to decide size of classification head.
    n_labels = len(labels_ids)

    print('Loading configuraiton...')
    model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)

    # Get model's tokenizer.
    print('Loading tokenizer...')
    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)
    # default to left padding
    tokenizer.padding_side = "left"
    # Define PAD Token = EOS Token = 50256
    tokenizer.pad_token = tokenizer.eos_token


    # Get the actual model.
    print('Loading model...')

    #creating global instance of the model
    global model
    model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)

    # resize model embedding to match new tokenizer
    model.resize_token_embeddings(len(tokenizer))

    # fix model padding token id
    model.config.pad_token_id = model.config.eos_token_id

    # Load model to defined device.
    model.to(device)
    print('Model loaded to `%s`'%device)

    # Create data collator to encode text and labels into numbers.
    gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer,
                                                          labels_encoder=labels_ids,
                                                          max_sequence_len=max_length)


    print('Dealing with Train...')
    # Create pytorch dataset.
    train_dataset = ToxicDataset(path=train_file,
                                  use_tokenizer=tokenizer)
    print('Created `train_dataset` with %d examples!'%len(train_dataset))

    # Move pytorch dataset into dataloader.
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)
    print('Created `train_dataloader` with %d batches!'%len(train_dataloader))

    print()

    print('Dealing with Validation...')
    # Create pytorch dataset.
    valid_dataset =  ToxicDataset(path=val_file,
                                  use_tokenizer=tokenizer)
    print('Created `valid_dataset` with %d examples!'%len(valid_dataset))

    # Move pytorch dataset into dataloader.
    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)
    print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))

    # Note: AdamW is a class from the huggingface library (as opposed to pytorch)
    # I believe the 'W' stands for 'Weight Decay fix"
    optimizer = AdamW(model.parameters(),
                      lr = 2e-5, # default is 5e-5, our notebook had 2e-5
                      eps = 1e-8 # default is 1e-8.
                      )

    # Total number of training steps is number of batches * number of epochs.
    # `train_dataloader` contains batched data so `len(train_dataloader)` gives
    # us the number of batches.
    total_steps = len(train_dataloader) * epochs

    # Create the learning rate scheduler.
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                num_warmup_steps = 0, # Default value in run_glue.py
                                                num_training_steps = total_steps)

    # Store the average loss after each epoch so we can plot them.
    all_loss = {'train_loss':[], 'val_loss':[]}
    all_acc = {'train_acc':[], 'val_acc':[]}

    # Loop through each epoch.
    print('Epoch')
    for epoch in tqdm(range(epochs)):
      print()
      print('Training on batches...')
      # Perform one full pass over the training set.
      train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)
      train_acc = accuracy_score(train_labels, train_predict)

      # Get prediction form model on validation data.
      print('Validation on batches...')
      valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)
      val_acc = accuracy_score(valid_labels, valid_predict)

      # Print loss and accuracy values to see how training evolves.
      print("  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f"%(train_loss, val_loss, train_acc, val_acc))
      print()

      # Store the loss value for plotting the learning curve.
      all_loss['train_loss'].append(train_loss)
      all_loss['val_loss'].append(val_loss)
      all_acc['train_acc'].append(train_acc)
      all_acc['val_acc'].append(val_acc)

    # Plot loss curves.
    plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])

    # Plot accuracy curves.
    plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])

    # Get prediction form model on validation data. This is where you should use
    # your test data.
    true_labels, predictions_labels, avg_epoch_loss = validation(valid_dataloader, device)

    # Create the evaluation report.
    evaluation_report = classification_report(true_labels, predictions_labels, labels=list(labels_ids.values()), target_names=list(labels_ids.keys()))
    # Show the evaluation report.
    print(evaluation_report)

    # Plot confusion matrix.
    plot_confusion_matrix(y_true=true_labels, y_pred=predictions_labels,
                        classes=list(labels_ids.keys()), normalize=True,
                        magnify=0.1,
                        );
    #compute performance
    try:
      compute_performance(true_labels,predictions_labels)
    except:
      print("An exception occurred with compute_performance")
    #save model

    save_model(model,model_dir)

    time.sleep(3) # wait for 3 seconds
    # Now Zip Model to share it
    zip_directory(model_dir, MODEL_Gen_File)

    time.sleep(3) # wait for 3 seconds
    model_gdrive_link = get_gdrive_link(MODEL_Gen_File)

    print(model_gdrive_link)
    link_model=get_shareable_link(model_gdrive_link)

    #save tokenizer
    save_model(tokenizer,token_dir)

    time.sleep(3) # wait for 3 seconds
    # Now Zip Model to share it
    zip_directory(token_dir, TOKEN_Gen_File)

    time.sleep(3) # wait for 3 seconds
    token_gdrive_link = get_gdrive_link(TOKEN_Gen_File)

    print(token_gdrive_link)
    link_token=get_shareable_link(token_gdrive_link)
    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################
    return link_model,link_token

"""## Testing Method 1 Code
Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  
"""

def test_Gen(test_file, MODEL_PATH,model_gdrive_link,token_gdrive_link):
    """
    take test_file, model_file and output_dir as input.
    It loads model and test of the examples in the test_file.
    It prints different evaluation metrics, and saves the output in output directory

    ADD Other arguments, if needed

    Args:
        test_file: test file name
        model_gdrive_link: GDrive URL
        MODEL_PATH: Directory of Model
        token_gdrive_link: GDrive URL

    """

    ##########################################################################
    #                     TODO: Implement this function                      #
    ##########################################################################
    # Replace "pass" statement with your code

    #download model from link
    model_file,token_file= download_model(MODEL_PATH,model_gdrive_link,token_gdrive_link)

    #load model
    # Load the tokenizer and model from the saved directory

    gpt_model, gpt_tokenizer = load_model(model_file,token_file)

    device = torch.device(
        'cuda') if torch.cuda.is_available() else torch.device('cpu')
    gpt_model=gpt_model.to(device)



    #load data
    test_data = pd.read_csv(test_file)

    print('Inferencing the Discriminative model')
    test_data['out_label_model_Gen'] = test_data['comment'].apply(lambda text: inference_gen(model=gpt_model,
                      tokenizer=gpt_tokenizer,
                      input_text=text,
                      device=device)[0].argmax(axis=-1)) #obtain the label
    # Now save the model output in the same output file
    test_data.to_csv(test_file, index=False)
    print('\n Output is saved in ', test_file)

    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################
    return

"""## Method Generative End

# Method Discriminative Start

In this section you will write all details of your Method 2.

You will have to enter multiple `code` and `text` cell.

Your code should follow the standard ML pipeline


*   Data reading
*   Data clearning, if any
*   Convert data to vector/tokenization/vectorization
*   Model Declaration/Initialization/building
*   Training and validation of the model using training and validation dataset
*   Save the trained model
*   Load and Test the model on testing set
*   Save the output of the model

You could add any other step(s) based on your method's requirement.

After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of `function`.

## Training Method Discriminative Code
Your test code should be a stand alone code that must take `train_file`, `val_file`,  and `model_dir` as input. You could have other things as also input, but these three are must. You would load both files, and train using the `train_file` and validating using the `val_file`. You will `print` / `display`/ `plot` all performance metrics, loss(if available) and save the output model in the `model_dir`.

Note that at the testing time, you need to use the same pre-processing and model. So, it would be good that you make those as seperate function/pipeline whichever it the best suited for your method. Don't copy-paste same code twice, make it a fucntion/class whichever is best.
"""

# code inspired from https://www.geeksforgeeks.org/toxic-comment-classification-using-bert/

def tokenize_and_encode(tokenizer, comments, labels, max_length=128):
    """
    Tokenizes and encodes comments using the provided tokenizer, adding special tokens,
    managing sequence length through truncation or padding, and generating attention masks.
    It is designed to prepare text data for input into models like BERT.

    Parameters:
    tokenizer: The tokenizer instance (e.g., from the Hugging Face Transformers library)
               used to tokenize and encode the text data.
    comments (list of str): The list of text comments to be tokenized and encoded.
    labels (list): The list of labels associated with the comments.
    max_length (int, optional): The maximum length of the sequence after tokenization and encoding.
                                Defaults to 128.

    Returns:
    tuple: A tuple containing three PyTorch tensors:
           - input_ids: Tensor of tokenized and encoded sequences
           - attention_masks: Tensor of attention masks to indicate padding
           - labels: Tensor of labels converted to float32 data type
    """
    # Initialize empty lists to store tokenized inputs and attention masks
    input_ids = []
    attention_masks = []

    # Iterate through each comment in the 'comments' list
    for comment in comments:
        # Tokenize and encode the comment using the tokenizer
        encoded_dict = tokenizer.encode_plus(
            comment,
            add_special_tokens=True,  # Add special tokens like [CLS] and [SEP]
            max_length=max_length,  # Truncate or pad to 'max_length'
            pad_to_max_length=True,  # Pad with zeros if needed
            return_attention_mask=True,  # Return attention mask
            return_tensors='pt'  # Return PyTorch tensors
        )

        # Append the tokenized input and attention mask to their respective lists
        input_ids.append(encoded_dict['input_ids'])
        attention_masks.append(encoded_dict['attention_mask'])

    # Concatenate the lists of tokenized inputs and attention masks into tensors
    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)

    # Convert the list of labels into a PyTorch tensor
    labels = torch.tensor(labels, dtype=torch.float32)

    # Return the tokenized inputs, attention masks, and labels as tensors
    return input_ids, attention_masks, labels

def inference_dis(input_text, model, tokenizer, device):
    """
    Performs inference on a given text input using the specified model and tokenizer.
    The text is first cleaned, then tokenized, and passed through the model to obtain predictions.

    Parameters:
    input_text (str): The text input for which the inference is to be performed.
    model: The pre-trained model used for inference (e.g., a BERT model).
    tokenizer: The tokenizer corresponding to the model, used for text preprocessing.
    device: The device (e.g., 'cuda' or 'cpu') on which the model is to run.

    Returns:
    numpy.ndarray: An array containing the model's predictions for each class in the input text.

    The function processes the input text through several steps: cleaning, tokenizing,
    creating a DataLoader for the input, and then running the model to get the logits.
    It applies a sigmoid function to the logits to obtain prediction probabilities.
    """
    # Clean the input text
    user_input = clean_text(input_text)

    # Tokenize the cleaned text
    user_encodings = tokenizer(
        user_input, truncation=True, padding=True, return_tensors="pt")

    # Create a dataset and loader for the tokenized text
    user_dataset = TensorDataset(
        user_encodings['input_ids'], user_encodings['attention_mask'])

    user_loader = DataLoader(user_dataset, batch_size=1, shuffle=False)

    # Perform inference
    model.eval()
    with torch.no_grad():
        for batch in user_loader:
            input_ids, attention_mask = [t.to(device) for t in batch]
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            predictions = torch.sigmoid(logits)

    # Convert the predictions to a numpy array and return
    predicted_labels = predictions.cpu().numpy()[0]
    return predicted_labels

def train_dis(train_file, val_file, model_dir,token_dir):
    """
    Takes train_file, val_file and model_dir as input.
    It trained on the train_file datapoints, and validate on the val_file datapoints.
    While training and validating, it print different evaluataion metrics and losses, wheverever necessary.
    After finishing the training, it saved the best model in the model_dir.

    ADD Other arguments, if needed.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory
        token_dir: Tokenizer output Directory

    """

    ##########################################################################
    #                     TODO: Implement this function                      #
    ##########################################################################
    # Replace "pass" statement with your code

    # Token Initialization
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased',
										do_lower_case=True)

    # Model Initialization
    model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',
													num_labels=1)

    # Move model to GPU if available
    device = torch.device(
      'cuda') if torch.cuda.is_available() else torch.device('cpu')
    model = model.to(device)

    #load data
    train = load_data(train_file)
    validation = load_data(val_file)
    #unmap the label
    train['toxicity'] = train['toxicity'].map({'no toxic':0,'toxic':1})
    validation['toxicity'] = validation['toxicity'].map({'no toxic':0,'toxic':1})

    # Tokenize and Encode the comments and labels for the training set
    input_ids, attention_masks, labels = tokenize_and_encode(
      tokenizer,
      train.comment.values,
      train.toxicity.values
    )

    # Tokenize and Encode the comments and labels for the validation set
    val_input_ids, val_attention_masks, val_labels = tokenize_and_encode(
      tokenizer,
      validation.comment.values,
      validation.toxicity.values
    )


    print('Training Comments :',train.shape)
    print('Input Ids		 :',input_ids.shape)
    print('Attention Mask :',attention_masks.shape)
    print('Labels		 :',labels.shape)

    # Creating DataLoader for the  dataset
    batch_size = 32
    train_dataset = TensorDataset(input_ids, attention_masks, labels)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # validation set
    val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # Optimizer setup
    optimizer = AdamW(model.parameters(), lr=2e-5)

    # Function to Train the Model
    def train_model(model, train_loader, optimizer, device, num_epochs):
      # Loop through the specified number of epochs
      for epoch in range(num_epochs):
        # Set the model to training mode
        model.train()
        # Initialize total loss for the current epoch
        total_loss = 0

        # Loop through the batches in the training data
        for batch in train_loader:
          input_ids, attention_mask, labels = [t.to(device) for t in batch]

          optimizer.zero_grad()

          outputs = model(
            input_ids, attention_mask=attention_mask, labels=labels)
          loss = outputs.loss
          total_loss += loss.item()

          loss.backward()
          optimizer.step()

        model.eval() # Set the model to evaluation mode
        val_loss = 0

        # Disable gradient computation during validation
        with torch.no_grad():
          for batch in val_loader:
            input_ids, attention_mask, labels = [
              t.to(device) for t in batch]

            outputs = model(
              input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            val_loss += loss.item()
        # Print the average loss for the current epoch
        print(
          f'Epoch {epoch+1}, Training Loss: {total_loss/len(train_loader)},Validation loss:{val_loss/len(val_loader)}')


    # Call the function to train the model
    train_model(model, train_loader, optimizer, device, num_epochs=3)

    # Evaluate the Model
    def evaluate_model(model, test_loader, device):
      model.eval() # Set the model to evaluation mode

      true_labels = []
      predicted_probs = []

      with torch.no_grad():
        for batch in test_loader:
          input_ids, attention_mask, labels = [t.to(device) for t in batch]

          # Get model's predictions
          outputs = model(input_ids, attention_mask=attention_mask)
          # Use sigmoid for multilabel classification
          predicted_probs_batch = torch.sigmoid(outputs.logits)
          predicted_probs.append(predicted_probs_batch.cpu().numpy())

          true_labels_batch = labels.cpu().numpy()
          true_labels.append(true_labels_batch)

      # Combine predictions and labels for evaluation
      true_labels = np.concatenate(true_labels, axis=0)
      predicted_probs = np.concatenate(predicted_probs, axis=0)

      predicted_labels = (predicted_probs > 0.6).astype(
        int) # Apply threshold for binary classification

      # Calculate evaluation metrics
      accuracy = accuracy_score(true_labels, predicted_labels)
      precision = precision_score(true_labels, predicted_labels, average='micro')
      recall = recall_score(true_labels, predicted_labels, average='micro')

      # Print the evaluation metrics
      print(f'Accuracy: {accuracy:.4f}')
      print(f'Precision: {precision:.4f}')
      print(f'Recall: {recall:.4f}')

      # Create the evaluation report.
      evaluation_report = classification_report(true_labels, predicted_labels)
      # Show the evaluation report.
      print(evaluation_report)
      try:
        # Plot confusion matrix.
        plot_confusion_matrix(y_true=true_labels, y_pred=predicted_labels,
                              classes=[0,1], normalize=True,
                              magnify=3,
                              );
      except:
        print('can not plot confusion matrix')
      compute_performance(true_labels,predicted_labels)

    # Call the function to evaluate the model on the test data
    evaluate_model(model, val_loader, device)

    # Model is working fine, so save model
    # Note modify this with your code
    save_model(model,model_dir)

    time.sleep(3)
    # Now Zip Model to share it
    zip_directory(model_dir, MODEL_Dis_File)

    model_gdrive_link = get_gdrive_link(MODEL_Dis_File)

    print(model_gdrive_link)
    model_link=get_shareable_link(model_gdrive_link)

    save_model(tokenizer,token_dir)
    time.sleep(3)
    # # Now Zip Model to share it
    zip_directory(token_dir, TOKEN_Dis_File)

    token_gdrive_link = get_gdrive_link(TOKEN_Dis_File)

    print(token_gdrive_link)
    token_link=get_shareable_link(token_gdrive_link)

    device = torch.device(
        'cuda') if torch.cuda.is_available() else torch.device('cpu')
    bert_model=bert_model.to(device)
    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################
    return model_link,token_link

"""## Testing Method Discriminative Code
Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  
"""

def test_dis(test_file, MODEL_PATH,model_gdrive_link,token_gdrive_link):
    """
    take test_file, model_file and output_dir as input.
    It loads model and test of the examples in the test_file.
    It prints different evaluation metrics, and saves the output in output directory

    ADD Other arguments, if needed

    Args:
        test_file: test file name
        model_gdrive_link: GDrive URL
        MODEL_PATH: Directory of Model
        token_gdrive_link: GDrive URL

    """

    ##########################################################################
    #                     TODO: Implement this function                      #
    ##########################################################################
    # Replace "pass" statement with your code

    #download model from link
    model_file,token_file= download_model(MODEL_PATH,model_gdrive_link,token_gdrive_link)

    #load model
    # Load the tokenizer and model from the saved directory

    bert_model, bert_tokenizer = load_model(model_file,token_file)

    device = torch.device(
        'cuda') if torch.cuda.is_available() else torch.device('cpu')
    bert_model=bert_model.to(device)


    #load data
    test_data = pd.read_csv(test_file)

    print('Inferencing the Discriminative model')
    test_data['out_label_model_Dis'] = test_data['comment'].apply(lambda text: inference_dis(model=bert_model,
                      tokenizer=bert_tokenizer,
                      input_text=text,
                      device=device)[0])

    test_data['out_label_model_Dis']= (test_data['out_label_model_Dis'].values > 0.6).astype(
        int) #apply treshold
    # Now save the model output in the same output file
    test_data.to_csv(test_file, index=False)
    print('\n Output is saved in ', test_file)

    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################
    return test_file

"""## Discriminative Method  End

# Other Method/model Start

## Combining generative and dicriminative models
Combining generative and discriminative models to build a more powerful classifier can be done by leveraging the strengths of each approach.

**Ensemble Method**

*Stacking:* Use the predictions of the GPT-2 and BERT models as input features to a secondary model, which then makes the final prediction. This secondary model will be a simple logistic regression model that learns how to best combine the predictions from the two models.
"""

import numpy as np
import seaborn as sns
from scipy.special import softmax
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

"""### Train stacking model"""

def train_Comb(train_file, val_file, MODEL_PATH, model_gen_gdrive_link, token_gen_gdrive_link, model_dis_gdrive_link, token_dis_gdrive_link,model_dir):
    """
    Trains a combined model using both generative and discriminative models on the provided training data.
    It uses a logistic regression model for stacking the predictions from both models to make the final prediction.

    Parameters:
    train_file (str): Path to the training data file.
    val_file (str): Path to the validation data file.
    MODEL_PATH (str): Path to save the trained model.
    model_gen_gdrive_link (str): Google Drive link to the generative model.
    token_gen_gdrive_link (str): Google Drive link to the tokenizer for the generative model.
    model_dis_gdrive_link (str): Google Drive link to the discriminative model.
    token_dis_gdrive_link (str): Google Drive link to the tokenizer for the discriminative model.

    Operations:
    - Sets the computation device based on availability (CPU/GPU).
    - Downloads and loads both generative and discriminative models along with their tokenizers.
    - Performs inference on the training data to get predictions from both models.
    - Combines the predictions using logistic regression.
    - Performs grid search to find the best hyperparameters for the logistic regression model.
    - Saves the combined model and provides a download link.

    Returns:
    str: A shareable link to the trained combined model stored on Google Drive.

    This function also evaluates the model on the validation data, printing the classification report
    and displaying a confusion matrix.
    """
    #set device
    device = torch.device(
            'cuda') if torch.cuda.is_available() else torch.device('cpu')

    #download gen model
    model_gen_file,token_gen_file= download_model(MODEL_PATH,model_gen_gdrive_link,token_gen_gdrive_link)

    # Load the tokenizer and model from the saved directory
    gpt_model, gpt_tokenizer = load_model(model_gen_file,token_gen_file)
    gpt_model=gpt_model.to(device)

    #download dis model
    model_file,token_file= download_model(MODEL_PATH,model_dis_gdrive_link,token_dis_gdrive_link)

    # Load the tokenizer and model from the saved directory
    bert_model, bert_tokenizer = load_model(model_file,token_file)
    bert_model=bert_model.to(device)

    #load data
    train_data = load_data(train_file)

    print('Inferencing generative model for obtaining training data this may take a while')
    train_data['gpt'] = train_data['comment'].apply(lambda text: inference_gen(model=gpt_model,
                          tokenizer=gpt_tokenizer,
                          input_text=text,
                          device=device)[0]) #obtain the logits

    print('Inferencing discriminative model for obtaining training data this may take a while')
    train_data['bert'] = train_data['comment'].apply(lambda text: inference_dis(model=bert_model,
                          tokenizer=bert_tokenizer,
                          input_text=text,
                          device=device)[0]) #obtain the probability

    # Convert GPT-2 logits to probabilities using softmax
    gpt2_probs = train_data['gpt'].apply(lambda logit: softmax(logit)[1])  # Softmax on logits and select positive class

    # For BERT, use the probabilities for the positive class directly
    bert_probs = train_data['bert']

    # Stack the probabilities for stacking model
    stacking_features = np.vstack((gpt2_probs, bert_probs)).T  # Nx2 matrix

    #define the model
    model  = LogisticRegression(random_state = student_id,class_weight='balanced')

    #define the parameter grid
    param_grid = {
        'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength
        'penalty': ['l1', 'l2'],  # Type of regularization
        'solver': ['liblinear', 'saga']  # Solvers
    }
    # Setup the grid search
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_macro', verbose=1)

    # Fit the grid search to the data
    grid_search.fit(stacking_features, train_data['toxicity'])

    # Best parameters and best score
    print("Best parameters:", grid_search.best_params_)
    print("Best score:", grid_search.best_score_)

    # Create a DataFrame
    cv_results = grid_search.cv_results_
    df = pd.DataFrame(cv_results)

    # Reshape for heatmap (pivot)
    df_pivot = df.pivot_table(
      index='param_C', columns='param_penalty', values='mean_test_score'
    )

    # Heatmap with annotations
    sns.heatmap(df_pivot, annot=True, fmt='.4f', cmap='viridis')

    #save model
    save_model_combine(grid_search,model_dir)

    zip_directory(model_dir, MODEL_Combine_File)

    time.sleep(3) # wait for 3 seconds
    model_gdrive_link = get_gdrive_link(MODEL_Combine_File)

    print(model_gdrive_link)
    link_model=get_shareable_link(model_gdrive_link)

    # For predictions, convert logits to probabilities for GPT-2, and stack with BERT probabilities
    val_data = load_data(val_file)

    print('Inferencing generative model for obtaining training data this may take a while...')
    val_data['gpt'] = val_data['comment'].apply(lambda text: inference_gen(model=gpt_model,
                          tokenizer=gpt_tokenizer,
                          input_text=text,
                          device=device)[0]) #obtain the logits

    print('Inferencing discriminative model for obtaining training data this may take a while...')
    val_data['bert'] = val_data['comment'].apply(lambda text: inference_dis(model=bert_model,
                          tokenizer=bert_tokenizer,
                          input_text=text,
                          device=device)[0]) #obtain the probability

    val_gpt_probs = val_data['gpt'].apply(lambda logit: softmax(logit)[1])

    test_stacking_features = np.vstack((val_gpt_probs, val_data['bert'])).T

    # Make final predictions with the logistic regression model
    final_predictions = grid_search.predict(test_stacking_features)

    #evaluate model
    print(classification_report(val_data['toxicity'],final_predictions))

    plot_confusion_matrix(y_true=val_data['toxicity'], y_pred=final_predictions,
                        classes=['no toxic','toxic'],normalize=True,
                        magnify=0.1,
                        );

    compute_performance(val_data['toxicity'],final_predictions)

    return link_model

"""### Test stacking model"""

def test_comb(test_file, MODEL_PATH,model_dis_gdrive_link,token_dis_gdrive_link,model_gen_gdrive_link,token_gen_gdrive_link,model_gdrive_link):
    """
    take test_file, model_file and output_dir as input.
    It loads model and test of the examples in the test_file.
    It prints different evaluation metrics, and saves the output in output directory

    ADD Other arguments, if needed

    Args:
        test_file: test file name
        model_gdrive_link: GDrive URL
        MODEL_PATH: Directory of Model

    """

    ##########################################################################
    #                     TODO: Implement this function                      #
    ##########################################################################

    #set device
    device = torch.device(
            'cuda') if torch.cuda.is_available() else torch.device('cpu')
    print('Downloading the models...')

    #download gen model
    model_gen_file,token_gen_file= download_model(MODEL_PATH,model_gen_gdrive_link,token_gen_gdrive_link)

    # Load the tokenizer and model from the saved directory
    gpt_model, gpt_tokenizer = load_model(model_gen_file,token_gen_file)
    gpt_model=gpt_model.to(device)

    #download dis model
    model_file,token_file= download_model(MODEL_PATH,model_dis_gdrive_link,token_dis_gdrive_link)

    # Load the tokenizer and model from the saved directory
    bert_model, bert_tokenizer = load_model(model_file,token_file)
    bert_model=bert_model.to(device)

    #download model from link
    model_file= download_combined_model(MODEL_PATH,model_gdrive_link)

    #load model

    model = load_model_combine(model_file)

    #load data
    data = pd.read_csv(test_file)

    test_data = data.copy()

    print('Inferencing generative model for obtaining training data this may take a while...')
    test_data['gpt'] = test_data['comment'].apply(lambda text: inference_gen(model=gpt_model,
                          tokenizer=gpt_tokenizer,
                          input_text=text,
                          device=device)[0]) #obtain the logits

    print('Inferencing discriminative model for obtaining training data this may take a while...')
    test_data['bert'] = test_data['comment'].apply(lambda text: inference_dis(model=bert_model,
                          tokenizer=bert_tokenizer,
                          input_text=text,
                          device=device)[0]) #obtain the probability

    gpt_probs = test_data['gpt'].apply(lambda logit: softmax(logit)[1])

    test_stacking_features = np.vstack((gpt_probs, test_data['bert'])).T

    # Make final predictions with the logistic regression model
    final_predictions = model.predict(test_stacking_features)

    data['out_label_model_Comb']=final_predictions

    data['out_label_model_Comb']=data['out_label_model_Comb'].map({'no toxic':0,'toxic':1})
    # Now save the model output in the same output file
    data.to_csv(test_file, index=False)
    print('\n Output is saved in ', test_file)

    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################
    return test_file

"""### Final ensamble models inference pipeline"""

def inference_stacked(text,MODEL_PATH,model_dis_gdrive_link,token_dis_gdrive_link,model_gen_gdrive_link,token_gen_gdrive_link,model_comb_gdrive_link):
    #set device
    device = torch.device(
            'cuda') if torch.cuda.is_available() else torch.device('cpu')

    #download gen model
    model_gen_file,token_gen_file= download_model(MODEL_PATH,model_gen_gdrive_link,token_gen_gdrive_link)

    # Load the tokenizer and model from the saved directory
    gpt_model, gpt_tokenizer = load_model(model_gen_file,token_gen_file)
    gpt_model=gpt_model.to(device)

    #download dis model
    model_file,token_file= download_model(MODEL_PATH,model_dis_gdrive_link,token_dis_gdrive_link)

    # Load the tokenizer and model from the saved directory
    bert_model, bert_tokenizer = load_model(model_file,token_file)
    bert_model=bert_model.to(device)

    #load model that combines generative and discriminative models to get more precision in final output

    #download model from link
    model_file= download_combined_model(MODEL_PATH,model_comb_gdrive_link)

    #load model

    model = load_model_combine(model_file)


    gpt_answer = inference_gen(model=gpt_model,
                          tokenizer=gpt_tokenizer,
                          input_text=text,
                          device=device)[0] #obtain the logits

    print('Inferencing discriminative model for obtaining training data this may take a while...')
    bert_answer = inference_dis(model=bert_model,
                          tokenizer=bert_tokenizer,
                          input_text=text,
                          device=device)[0] #obtain the probability

    gpt_probs = softmax(gpt_answer)[1]

    test_stacking_features = np.vstack((gpt_probs, bert_answer)).T

    # Make final predictions with the logistic regression model
    final_answer = model.predict(test_stacking_features)

    print('This text is: ',final_answer[0])

"""## SoTA"""

pip install perspective

import ast

def read_txt_to_dataframe(filename):
    with open(filename, 'r') as file:
        data = [ast.literal_eval(line) for line in file]
    df = pd.DataFrame(data)
    return df

def eval_sota(sota_wiki_file,val_wiki_file,sota_file):

    #read kaggle validation sota predictions
    filename = sota_wiki_file
    sota_wiki = read_txt_to_dataframe(filename).rename({'toxicity':'prediction'},axis='columns')
    val_wiki = pd.read_csv(val_wiki_file)

    #merge predictions and Ground truth
    val_wiki=val_wiki.merge(sota_wiki,on='id',how='left').drop_duplicates().fillna(0)[['id','comment_text','toxic','prediction']]


    #read assignment validation sota predictions
    filename = sota_file
    sota = read_txt_to_dataframe(filename).rename({'toxicity':'prediction','id':'comment_id','comment':'comment_text'},axis='columns')
    val = pd.read_csv(val_file)

    #merge predictions and Ground truth
    val=val.merge(sota,on='comment_id',how='left').drop_duplicates().fillna(0)[['comment_id','comment','toxicity','prediction']]

    #transform probabilities to labels
    val_wiki['prediction_treshold']=val_wiki['prediction'].apply(lambda x: 0 if x<=0.35 else 1)
    val['prediction_treshold']=val['prediction'].apply(lambda x: 0 if x<=0.35 else 1)

    #evaluate kaggle dataset
    print(classification_report(val_wiki['toxic'],val_wiki['prediction_treshold']))
    plot_confusion_matrix(val_wiki['toxic'],val_wiki['prediction_treshold'],clases=[0,1],normalize=True)
    compute_performance(val_wiki['toxic'],val_wiki['prediction_treshold'])

    #evaluate assignment dataset
    print(classification_report(val['toxicity'],val['prediction_treshold']))
    plot_confusion_matrix(val['toxicity'],val['prediction_treshold'],clases=[0,1],normalize=True)
    compute_performance(val['toxicity'],val['prediction_treshold'])

    return None

"""## SHAP"""

# code inspired from https://colab.research.google.com/github/ml6team/quick-tips/blob/main/nlp/2021_04_22_shap_for_huggingface_transformers/explainable_transformers_using_shap.ipynb#scrollTo=9gtVw-CytV3A

pip install shap

import shap
from transformers import TextClassificationPipeline

def explainable_ai(text,model_gen_gdrive_link,token_gen_gdrive_link,model_dis_gdrive_link,token_dis_gdrive_link):
    #set device
    device = torch.device(
                'cuda') if torch.cuda.is_available() else torch.device('cpu')

        #download gen model
    model_gen_file,token_gen_file= download_model(MODEL_PATH,model_gen_gdrive_link,token_gen_gdrive_link)

        # Load the tokenizer and model from the saved directory
    gpt_model, gpt_tokenizer = load_model(model_gen_file,token_gen_file)
    gpt_model=gpt_model.to(device)

        #download dis model
    model_file,token_file= download_model(MODEL_PATH,model_dis_gdrive_link,token_dis_gdrive_link)

        # Load the tokenizer and model from the saved directory
    bert_model, bert_tokenizer = load_model(model_file,token_file)
    bert_model=bert_model.to(device)

    #inference pipeline
    bert_pipe = TextClassificationPipeline(model=bert_model, tokenizer=bert_tokenizer, return_all_scores=True)

    gpt_pipe = TextClassificationPipeline(model=gpt_model, tokenizer=gpt_tokenizer, return_all_scores=True)

    #plot functions
    def score_and_visualize_B(text):
      prediction = bert_pipe([text])
      print(prediction[0])

      explainer = shap.Explainer(bert_pipe)
      shap_values = explainer([text])

      shap.plots.text(shap_values)

    def score_and_visualize_G(text):
      prediction = gpt_pipe([text])
      print(prediction[0])

      explainer = shap.Explainer(gpt_pipe)
      shap_values = explainer([text])

      shap.plots.text(shap_values)

    score_and_visualize_B(text)
    score_and_visualize_G(text)

"""## Main app"""

import argparse

# Define argparse-like function
def parse_arguments(option):
    parser = argparse.ArgumentParser(description='Process some integers.')
    parser.add_argument('--option', '-o',  type=str, default=option, help='Description of your option.')
    args = parser.parse_args(args=[])
    return args

# Function to perform some action based on selected option
def perform_action(option):
    print("Performing action with option:", option)

    if option == '0':
      print('\n Okay Exiting!!! ')

    elif option == '1':
      print('\n Training Generative Model')
      upload_data(train_file,val_file,test_file,train2_file,train_wiki_file,val_wiki_file)
      time.sleep(3)
      model_gdrive_link,token_gdrive_link = train_Gen(train_wiki_file,val_wiki_file,MODEL_Gen_DIRECTORY,TOKEN_Gen_DIRECTORY)
      print('Make sure to pass model URL in Testing',model_gdrive_link,token_gdrive_link)

    elif option == '2':
      print('\n\n Pass the URL Not Variable !!!')
      print('\n Testing Generative Model')
      model_gen_url = 'https://drive.google.com/file/d/106zSjyKs0aF6emd5BRf27gzQGk6LucWl/view?usp=sharing'
      token_gen_url = 'https://drive.google.com/file/d/10876bUqoRCoyiKAQ4ERVgU9u3hpdDAVs/view?usp=sharing'
      test_Gen(test_file,MODEL_PATH, model_gen_url,token_gen_url)

    elif option == '3':
      print('\n Training Disciminative Model')
      upload_data(train_file,val_file,test_file,train2_file,train_wiki_file,val_wiki_file)
      time.sleep(3)
      model_gdrive_link,token_gdrive_link = train_dis(train_wiki_file,val_wiki_file,MODEL_Dis_DIRECTORY,TOKEN_Dis_DIRECTORY)
      print('Make sure to pass model URL in Testing',model_gdrive_link)
      print('Make sure to pass token URL in Testing',token_gdrive_link)

    elif option == '4':
      print('\n\n Pass the URL Not Variable !!!')
      print('\n Testing Disciminative Model')
      model_dis_url = 'https://drive.google.com/file/d/1-E9SGhIp-jFiUtTMS18cm6-gm7MOe4AN/view?usp=sharing'
      token_dis_url = 'https://drive.google.com/file/d/1-pvLu-JDcqikzkBXMEl2zPHeg_AQLK1g/view?usp=sharing'
      test_dis(test_file, MODEL_PATH, model_dis_url,token_dis_url)

    elif option == '5':
      print('\n Training Combination Model')
      upload_data(train_file,val_file,test_file,train2_file,train_wiki_file,val_wiki_file)
      time.sleep(3)
      model_gdrive_link,token_gdrive_link = train_dis(train_wiki_file,val_wiki_file,MODEL_Combine_DIRECTORY)
      print('Make sure to pass model URL in Testing',model_gdrive_link)
      print('Make sure to pass token URL in Testing',token_gdrive_link)

    elif option == '6':
      print('\n\n Pass the URL Not Variable !!!')
      print('\n Testing Combination Model')
      model_dis_url = 'https://drive.google.com/file/d/1-E9SGhIp-jFiUtTMS18cm6-gm7MOe4AN/view?usp=sharing'
      token_dis_url = 'https://drive.google.com/file/d/1-pvLu-JDcqikzkBXMEl2zPHeg_AQLK1g/view?usp=sharing'

      model_gen_url = 'https://drive.google.com/file/d/106zSjyKs0aF6emd5BRf27gzQGk6LucWl/view?usp=sharing'
      token_gen_url = 'https://drive.google.com/file/d/10876bUqoRCoyiKAQ4ERVgU9u3hpdDAVs/view?usp=sharing'

      model_comb_url = 'https://drive.google.com/file/d/1-1tLCM7fXvxQz_k_8baWtZbsl7nuSkJu/view?usp=sharing'

      test_dis(test_file, MODEL_PATH, model_dis_url,token_dis_url,model_gen_url,token_gen_url,model_comb_url)

    elif option == '7':
      print('\n\n Toxicity classifier')
      text = input('\n Write the text you want to classify: ')
      model_dis_url = 'https://drive.google.com/file/d/1-E9SGhIp-jFiUtTMS18cm6-gm7MOe4AN/view?usp=sharing'
      token_dis_url = 'https://drive.google.com/file/d/1-pvLu-JDcqikzkBXMEl2zPHeg_AQLK1g/view?usp=sharing'

      model_gen_url = 'https://drive.google.com/file/d/106zSjyKs0aF6emd5BRf27gzQGk6LucWl/view?usp=sharing'
      token_gen_url = 'https://drive.google.com/file/d/10876bUqoRCoyiKAQ4ERVgU9u3hpdDAVs/view?usp=sharing'

      model_comb_url = 'https://drive.google.com/file/d/1-1tLCM7fXvxQz_k_8baWtZbsl7nuSkJu/view?usp=sharing'
      inference_stacked(text, MODEL_PATH, model_dis_url,token_dis_url,model_gen_url,token_gen_url,model_comb_url)

    else:
      print('Wrong Option Selected. \n\nPlease select Correct option')
      main()

def main():

    # Get option from user input
    user_option = input("0. To Exit Code\n"
                     "1. Train Model Generative\n"
                    "2. Test Model Generative\n"
                    "3. Train Model Discriminative\n"
                    "4. Test Model Discriminative\n"
                    "5. Train Model Combining(generative + discriminative)\n"
                    "6. Test Model Combining(generative + discriminative)\n"
                    "7. Try the final model classifier in a text\n"
                    "Enter your option: ")

    args = parse_arguments(user_option)
    option = args.option
    perform_action(option)

main()

main()

"""##Other Method/model End"""
